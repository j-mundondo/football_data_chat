{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed successfully\n",
      "Preprocessing completed successfully\n",
      "Preprocessing completed successfully\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from custom_agents_LG import load_session_data, agent_executor\n",
    "from llm import get_llama_3dot3_70b_versatile,get_llama_3dot1_8b_instant,get_70b_8192\n",
    "from loaders_and_chroma_utils import vectorstore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "import streamlit as st\n",
    "import os\n",
    "llm_api_key = st.secrets['GROQ_API_KEY']\n",
    "langchain_api_key = st.secrets['LANGCHAIN_API_KEY']\n",
    "llm = get_70b_8192()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\j.mundondo\\AppData\\Local\\Temp\\ipykernel_27768\\1703591915.py:20: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader, \n",
    "    UnstructuredPowerPointLoader, \n",
    "    CSVLoader, \n",
    "    UnstructuredExcelLoader\n",
    ")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "\n",
    "# Initialize components\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Chroma with persistent client\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "vectorstore = Chroma(\n",
    "    client=client,\n",
    "    embedding_function=embedding_function,\n",
    "    collection_name=\"my_collection\"\n",
    ")\n",
    "\n",
    "# Define State class\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    player_metrics: str\n",
    "    answer: str\n",
    "    player_name: str\n",
    "\n",
    "# Document loading functions\n",
    "def load_and_split_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"Load and split a document into chunks.\"\"\"\n",
    "    if file_path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    elif file_path.endswith(('.ppt', '.pptx')):\n",
    "        loader = UnstructuredPowerPointLoader(file_path, mode=\"elements\")\n",
    "    elif file_path.endswith(('.xls', '.xlsx')):\n",
    "        loader = UnstructuredExcelLoader(file_path)\n",
    "    elif file_path.endswith('.csv'):\n",
    "        loader = CSVLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "    documents = loader.load()\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def verify_document_loading(file_path: str):\n",
    "    \"\"\"Load, index, and verify document loading with detailed debugging.\"\"\"\n",
    "    print(\"\\nSTEP 1: Loading document...\")\n",
    "    try:\n",
    "        splits = load_and_split_document(file_path)\n",
    "        print(f\"Successfully split document into {len(splits)} chunks\")\n",
    "        print(\"\\nFirst chunk preview:\")\n",
    "        if splits:\n",
    "            print(splits[0].page_content[:200])\n",
    "        \n",
    "        print(\"\\nSTEP 2: Clearing existing vectorstore...\")\n",
    "        existing_ids = vectorstore._collection.get()['ids']\n",
    "        if existing_ids:\n",
    "            vectorstore._collection.delete(ids=existing_ids)\n",
    "        print(\"Vectorstore cleared\")\n",
    "        \n",
    "        print(\"\\nSTEP 3: Indexing document chunks...\")\n",
    "        for i, split in enumerate(splits):\n",
    "            split.metadata['file_id'] = 1\n",
    "            split.metadata['chunk_id'] = i\n",
    "        \n",
    "        vectorstore.add_documents(splits)\n",
    "        print(f\"Added {len(splits)} chunks to vectorstore\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during document processing: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Graph nodes\n",
    "def extract_player_name(question: str) -> str:\n",
    "    \"\"\"Extract player name from the question using the LLM.\"\"\"\n",
    "    name_extraction_prompt = \"\"\"\n",
    "    Extract the player name(s) from the following question. \n",
    "    If no specific player is mentioned, return None.\n",
    "    Only return the name(s) without any additional text.\n",
    "    \n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(name_extraction_prompt)\n",
    "    messages = prompt.invoke({\"question\": question})\n",
    "    response = llm.invoke(messages).content.strip()\n",
    "    \n",
    "    return response if response.lower() != \"none\" else None\n",
    "\n",
    "def get_player_metrics(player_name: str) -> dict:\n",
    "    \"\"\"Get metrics for a specific player.\"\"\"\n",
    "    return agent_executor.invoke({\n",
    "        \"input\": f\"Get {player_name}'s metrics as a dictionary format\"\n",
    "    })[\"output\"]\n",
    "\n",
    "def setup_initial_state(question: str) -> State:\n",
    "    \"\"\"Setup initial state without metrics - moved to separate node.\"\"\"\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"context\": [],\n",
    "        \"player_metrics\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"player_name\": \"\"\n",
    "    }\n",
    "\n",
    "def extract_name_node(state: State):\n",
    "    \"\"\"Node for extracting player name.\"\"\"\n",
    "    player_name = extract_player_name(state[\"question\"])\n",
    "    return {\"player_name\": player_name}\n",
    "\n",
    "def get_metrics_node(state: State):\n",
    "    \"\"\"Node for getting player metrics.\"\"\"\n",
    "    if not state[\"player_name\"]:\n",
    "        return {\"player_metrics\": \"No specific player mentioned in query\"}\n",
    "    \n",
    "    metrics = get_player_metrics(state[\"player_name\"])\n",
    "    return {\"player_metrics\": metrics}\n",
    "\n",
    "def retrieve_node(state: State):\n",
    "    \"\"\"Node for retrieving relevant documents.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    if state[\"player_metrics\"] != \"No specific player mentioned in query\":\n",
    "        metrics_doc = Document(\n",
    "            page_content=f\"Current Player Metrics for {state['player_name']}:\\n{state['player_metrics']}\",\n",
    "            metadata={\"source\": \"player_metrics\"}\n",
    "        )\n",
    "        documents.append(metrics_doc)\n",
    "    \n",
    "    # Get research documents with specific queries\n",
    "    research_queries = [\n",
    "        f\"injury risk thresholds for {state['player_name']}'s activity pattern\",\n",
    "        \"critical thresholds for Dynamic Stress Load DSL\",\n",
    "        \"metabolic power thresholds for injury risk\",\n",
    "        \"recovery indicators and patterns research\",\n",
    "        \"acceleration deceleration ratio research findings\"\n",
    "    ]\n",
    "    \n",
    "    for query in research_queries:\n",
    "        retrieved_docs = vectorstore.similarity_search(\n",
    "            query,\n",
    "            k=2\n",
    "        )\n",
    "        documents.extend(retrieved_docs)\n",
    "    \n",
    "    return {\"context\": documents}\n",
    "\n",
    "def generate_node(state: State):\n",
    "    \"\"\"Generate analysis with explicit reference to research data.\"\"\"\n",
    "    if state[\"player_metrics\"] == \"No specific player mentioned in query\":\n",
    "        return {\"answer\": \"To analyze a player's injury risks, please specify a player name in your query.\"}\n",
    "    \n",
    "    # Separate metrics and research documents\n",
    "    metrics_doc = None\n",
    "    research_docs = []\n",
    "    \n",
    "    for doc in state[\"context\"]:\n",
    "        if doc.metadata.get(\"source\") == \"player_metrics\":\n",
    "            metrics_doc = doc\n",
    "        else:\n",
    "            research_docs.append(doc)\n",
    "    \n",
    "    # Format research findings\n",
    "    research_findings = \"\\n\\n\".join([\n",
    "        f\"RESEARCH FINDING {i+1}:\\n{doc.page_content}\"\n",
    "        for i, doc in enumerate(research_docs)\n",
    "    ])\n",
    "    \n",
    "    analysis_prompt = \"\"\"\n",
    "    Using ONLY the provided research data and player metrics, conduct an injury risk analysis.\n",
    "    \n",
    "    PLAYER METRICS:\n",
    "    {metrics}\n",
    "    \n",
    "    RESEARCH FINDINGS:\n",
    "    {research}\n",
    "    \n",
    "    Analyze the following aspects, citing ONLY the provided research:\n",
    "    1. Compare the player's metrics to the research thresholds\n",
    "    2. Identify specific risk factors supported by the research\n",
    "    3. Make recommendations based on the research findings\n",
    "    \n",
    "    Format your response with clear sections and evidence from the provided research.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = PromptTemplate.from_template(analysis_prompt).invoke({\n",
    "        \"metrics\": metrics_doc.page_content if metrics_doc else \"No metrics available\",\n",
    "        \"research\": research_findings\n",
    "    })\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "def build_streaming_graph():\n",
    "    \"\"\"Build graph with streaming support.\"\"\"\n",
    "    workflow = StateGraph(State)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"extract_name\", extract_name_node)\n",
    "    workflow.add_node(\"get_metrics\", get_metrics_node)\n",
    "    workflow.add_node(\"retrieve\", retrieve_node)\n",
    "    workflow.add_node(\"generate\", generate_node)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(\"extract_name\", \"get_metrics\")\n",
    "    workflow.add_edge(\"get_metrics\", \"retrieve\")\n",
    "    workflow.add_edge(\"retrieve\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"extract_name\")\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def analyze_player_risk_streaming(question: str):\n",
    "    \"\"\"Stream the analysis process.\"\"\"\n",
    "    graph = build_streaming_graph()\n",
    "    initial_state = setup_initial_state(question)\n",
    "    \n",
    "    print(\"Starting Analysis Stream:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for step in graph.stream(initial_state, stream_mode=\"updates\"):\n",
    "        print(\"Step Update:\")\n",
    "        for key, value in step.items():\n",
    "            if key == \"context\":\n",
    "                print(f\"{key}: {len(value)} documents retrieved\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return graph.invoke(initial_state)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # First load and verify research document\n",
    "    file_path = r\"C:\\Users\\j.mundondo\\OneDrive - Statsports\\Desktop\\statsportsdoc\\Projects\\frequency_chat_PH\\data\\multi_session_hias\\Research Paper.docx\"\n",
    "    success = verify_document_loading(file_path)\n",
    "    \n",
    "    if success:\n",
    "        # Test queries\n",
    "        test_queries = [\n",
    "            \"What are Lee's injury risks based on his recent activity pattern?\",\n",
    "            \"Analyze injury risks for Hawk\",\n",
    "            \"What are the injury risks for this player's recent pattern?\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\nAnalyzing query: {query}\")\n",
    "            print(\"-\" * 50)\n",
    "            response = analyze_player_risk_streaming(query)\n",
    "            print(\"Analysis Results:\")\n",
    "            print(response[\"answer\"])\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "footydata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
